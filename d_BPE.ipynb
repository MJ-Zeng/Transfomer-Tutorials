{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# BPE\n",
    "\n",
    "---\n",
    "\n",
    "## 一、OOV 问题的核心挑战\n",
    "\n",
    "**OOV（Out-of-Vocabulary，词汇库外）问题**是自然语言处理（NLP）的核心痛点，指模型在推理或生成过程中遇到 **训练数据中未出现的新词或生僻词**，导致无法正确识别或处理这些词汇的问题。\n",
    "\n",
    "### **典型场景**\n",
    "- **新词涌现**：网络热词（如“内卷”“躺平”）、专业术语（如“元宇宙”）等不断产生；\n",
    "- **低频词问题**：训练语料中低频词（如“unhappiness”）未被纳入词表；\n",
    "- **形态多样性**：词形变化（如“run”“running”“runner”）增加词表复杂度；\n",
    "- **多语言差异**：通用词表难以覆盖不同语言的构词规则。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、传统分词方案的局限性\n",
    "\n",
    "| **分词方案**         | **优点**                                                                 | **缺点**                                                                 | **适用场景**                     |\n",
    "|----------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|----------------------------------|\n",
    "| **字符级**           | 词汇表极小（如英文 26 字符 + 符号），无未登录词（OOV）                   | 序列过长（语义单位破碎），模型学习效率低                                 | 小语种、低资源语言               |\n",
    "| **词级**             | 语义单位完整，人类可解释性强                                             | 词汇表极大（如英文超百万），OOV 严重（生僻词、缩写、新词）               | 简单任务（如情感分析）、高资源语言 |\n",
    "| **子词级（如 BPE）** | 词汇表大小适中，OOV 少，语义单位更合理（如 “unhappiness” 拆为 “un-happi-ness”） | 需预先训练分词器，对低频子词仍可能处理不足                               | 预训练模型（BERT/GPT）、多语言任务 |\n",
    "\n",
    "---\n",
    "\n",
    "## 三、BPE 的核心原理\n",
    "\n",
    "BPE（Byte Pair Encoding）通过 **迭代合并高频字符对**，逐步生成子词单元，平衡词汇表大小与语义表达精度：\n",
    "\n",
    "1. **初始单位**：以字符为最小单位（如英文初始词汇表为所有字母、数字、标点符号）；\n",
    "2. **迭代合并**：扫描文本，统计相邻字符对的频率，合并最高频的对；\n",
    "3. **终止条件**：达到预设的词汇表大小或合并次数阈值。\n",
    "\n",
    "**效果**：\n",
    "- **高频词**直接合并为完整词（如“the”“and”）；\n",
    "- **中频词**拆为少量子词（如“apple”→“app”+“le”）；\n",
    "- **低频词**拆为更多子词（如“unhappiness”→“un”+“happi”+“ness”），显著减少 OOV。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、BPE 与 Zipf 定律的关系\n",
    "\n",
    "### **Zipf 定律简介**\n",
    "- **定义**：自然语言中，词频与词的排序满足幂律分布：\n",
    "  $$\n",
    "  \\text{频率} \\propto \\frac{1}{\\text{词序}}\n",
    "  $$\n",
    "- **现象**：高频词数量少（如“the”“be”），低频词数量多（如“unhappiness”）。\n",
    "\n",
    "![Zipf Law示意图](img/4_1_zipfs-law.png)\n",
    "\n",
    "### **BPE 如何利用 Zipf 定律**\n",
    "- **高频词优先合并**：通过合并高频字符对（如“the”“and”），快速构建常用词；\n",
    "- **低频词拆分处理**：低频词因 Zipf 定律天然稀疏，BPE 通过子词单元（如“un-happi-ness”）减少 OOV；\n",
    "- **词汇表压缩**：Zipf 定律表明高频词贡献主要信息量，BPE 通过控制词汇表大小（如 30k~50k）保留高频词，舍弃低频词，实现高效建模。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、BPE 的执行流程（简要）\n",
    "\n",
    "### **步骤 1：准备训练数据**\n",
    "训练文本：\n",
    "```\n",
    "\"low low low lowly lower newer newer\"\n",
    "```\n",
    "预处理后：\n",
    "- `low</w>`: 3 次\n",
    "- `lowly</w>`: 1 次\n",
    "- `lower</w>`: 1 次\n",
    "- `newer</w>`: 2 次\n",
    "\n",
    "---\n",
    "\n",
    "### **步骤 2：统计子词对频率**\n",
    "| **子词对**       | **总频率** |\n",
    "|------------------|------------|\n",
    "| `(l,o)`          | 5          |\n",
    "| `(o,w)`          | 5          |\n",
    "| `(w,e)`          | 3          |\n",
    "| `(e,r)`          | 3          |\n",
    "| `(r,</w>)`       | 3          |\n",
    "\n",
    "---\n",
    "\n",
    "### **步骤 3：迭代合并子词对**\n",
    "1. **第一次合并**：`(l,o)` → `lo`\n",
    "   - `low</w>` → `lo + w + </w>`\n",
    "2. **第二次合并**：`(lo,w)` → `low`\n",
    "   - `low</w>` → `low + </w>`\n",
    "3. **第三次合并**：`(low,</w>)` → `low</w>`\n",
    "   - `low</w>` → `low</w>`\n",
    "4. **第四次合并**：`(e,r)` → `er`\n",
    "   - `lower</w>` → `low + e + er + </w>`\n",
    "\n",
    "---\n",
    "\n",
    "## 六、BPE 的解码过程\n",
    "\n",
    "BPE 解码是 **分词的逆过程**，规则如下：\n",
    "1. **直接拼接**所有子词；\n",
    "2. **遇到终止符 `</w>`** 时，替换为空格；\n",
    "3. **去除多余空格**，得到原始句子。\n",
    "\n",
    "**示例**：\n",
    "- 子词序列 `[low</w>, low, l, y</w>]`\n",
    "  解码后：`low lowly`\n",
    "\n",
    "---\n",
    "\n",
    "## 七、BPE 的优缺点总结\n",
    "\n",
    "### **优点**\n",
    "- **控制词汇表大小**：避免词级分词的 “百万级词汇表”，降低模型参数量和训练成本；\n",
    "- **减少 OOV 问题**：低频词、新词可拆分为已有子词，几乎无未登录词；\n",
    "- **语义连贯性**：合并的子词通常具有语义关联（如 “un-”“-ness”），帮助模型学习词法规律；\n",
    "- **多语言适配**：无需针对不同语言设计特殊分词规则（如中文可直接以字符为初始单位）。\n",
    "\n",
    "### **缺点**\n",
    "- **依赖训练数据**：若训练数据覆盖不足，可能生成不合理的子词（如低频词拆分为过细的字符）；\n",
    "- **固定合并规则**：一旦训练完成，分词规则固定，无法动态适应新领域的词汇（需重新训练分词器）；\n",
    "- **处理效率**：长文本分词时，需遍历子词对统计频率，效率低于词级分词。"
   ],
   "id": "9df360a61a220717"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 九、BPE代码实现\n",
    "### 9.1  Learn a variable-length encoding of the vocabulary in a text"
   ],
   "id": "c8e7997148081356"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T07:03:09.375977Z",
     "start_time": "2025-09-05T07:03:09.370940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import codecs\n",
    "import re\n",
    "import copy\n",
    "import warnings\n",
    "from collections import defaultdict, Counter"
   ],
   "id": "5dc3ccd7d1323f3f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T07:03:09.449255Z",
     "start_time": "2025-09-05T07:03:09.421587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_vocabulary(vocab, file_name, is_dict=False):\n",
    "    \"\"\"\n",
    "    统计文本文件中的词汇频率，更新词汇表字典。\n",
    "\n",
    "    参数:\n",
    "        vocab (dict 或 defaultdict(int)): 待更新的词汇表。若为普通字典，新单词默认计数为1。\n",
    "        file_name (str): 输入文件路径。\n",
    "        is_dict (bool): 若为 True，每行格式为 \"word count\"；若为 False，每行按空白分割单词。\n",
    "\n",
    "    返回:\n",
    "        defaultdict(int): 更新后的词汇表（键：单词，值：频率计数）。\n",
    "    \"\"\"\n",
    "    # 转换为 defaultdict(int) 以兼容普通字典输入\n",
    "    if not isinstance(vocab, defaultdict):\n",
    "        vocab = defaultdict(int, vocab)\n",
    "\n",
    "    # 使用 utf-8-sig 自动处理 BOM\n",
    "    with open(file_name, 'r', encoding='utf-8-sig') as fobj:\n",
    "        for line_num, line in enumerate(fobj, 1):  # 行号从1开始\n",
    "            line = line.strip('\\r\\n')  # 去除首尾换行符\n",
    "            if not line:\n",
    "                continue  # 跳过空行\n",
    "\n",
    "            if is_dict:\n",
    "                parts = line.split()\n",
    "                if len(parts) != 2:\n",
    "                    print(f\"警告：第 {line_num} 行格式错误（预期 'word count'）: {line}\")\n",
    "                    continue\n",
    "                word, count_str = parts\n",
    "                try:\n",
    "                    count = int(count_str)\n",
    "                except ValueError as e:\n",
    "                    print(f\"警告：第 {line_num} 行计数转换失败: {count_str}。错误: {e}\")\n",
    "                    continue\n",
    "                vocab[word] += count\n",
    "            else:\n",
    "                # 按任意空白分割单词（处理多空格、制表符等）\n",
    "                words = line.split()\n",
    "                for word in words:\n",
    "                    vocab[word] += 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def update_pair_statistics(pair, changed, stats, indices):\n",
    "    \"\"\"\n",
    "    更新符号对的索引和频率\n",
    "    :param pair: (str,str) 当前要合并的符号对（‘a’,b）\n",
    "    :param changed: List[(j,word,old_word,freq)]\n",
    "    :param stats: defaultdict(int) (key:符号对，val：频率)\n",
    "    :param indices: 符号对索引字典:记录该符号对在哪些单词中出现及其次数）。\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # step 1: 初始化当前符号对的统计信息\n",
    "    stats[pair] = 0\n",
    "    indices[pair] = defaultdict(int)\n",
    "\n",
    "    # step 2: 分解当前符号对\n",
    "    first, second = pair\n",
    "    new_pair = first + second\n",
    "\n",
    "    # step 3 :遍历所有受影响的单词\n",
    "    for j, word, old_word, freq in changed:\n",
    "        # step 3.1: 处理旧单词\n",
    "        i = 0\n",
    "        try:\n",
    "            i = old_word.index(first, i)\n",
    "        except ValueError as e:\n",
    "            break\n",
    "        if i < len(old_word) - 1 and old_word[i + 1] == second:\n",
    "            # 处理前一个符号对（如 A B → 合并 B C 后，A B 的频率减少）\n",
    "            if i:\n",
    "                prev = old_word[i - 1:i + 1]\n",
    "                stats[prev] -= freq\n",
    "                indices[prev][j] -= 1\n",
    "            # 处理后一个符号对（如 B C B → 合并 B C 后，C B 的频率减少）\n",
    "            if i < len(old_word) - 2:\n",
    "                if old_word[i + 2] != first or i >= len(old_word) - 3 or old_word[i + 3] != second:\n",
    "                    nex = old_word[i + 1:i + 3]\n",
    "                    stats[nex] -= freq\n",
    "                    indices[nex][j] -= 1\n",
    "\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "        i = 0\n",
    "        # step 4 :处理新词\n",
    "        while True:\n",
    "            try:\n",
    "                i = word.index(new_pair, i)\n",
    "            except ValueError as e:\n",
    "                break\n",
    "            # 处理前一个符号对（如 A BC → 合并 B C 后，A BC 的频率增加）\n",
    "            if i:\n",
    "                prev = word[i - 1, i + 1]\n",
    "                stats[prev] += freq\n",
    "                indices[prev][j] += 1\n",
    "            # 处理后一个符号对（如 BC B → 合并 B C 后，BC B 的频率增加）\n",
    "            if i < len(word) - 1 and word[i + 1] != new_pair:\n",
    "                nex = word[i:i + 2]\n",
    "                stats[nex] += freq\n",
    "                indices[nex][j] += 1\n",
    "            i += 1\n",
    "\n",
    "\n",
    "def get_pair_statistic(vocab):\n",
    "    \"\"\"\n",
    "    统计符号对频率和位置\n",
    "    :param vocab:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 符号对频率统计（键：符号对，值：频率）\n",
    "    stats = defaultdict(int)\n",
    "    # 符号对索引（键：符号对，值：{单词索引: 出现次数}）\n",
    "    indices = defaultdict(lambda: defaultdict(int))\n",
    "    # 单词的第一个字符开始，依次与下一个字符组成相邻符号对（如 word=('a','b','c')会生成 (a,b)和 (b,c)\n",
    "    for i, (word, freq) in enumerate(vocab):\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            stats[prev_char, char] += freq\n",
    "            # indices[('A', 'B')] = {\n",
    "            #     0: 5,  # 符号对 (A,B) 在单词0中出现5次（或1次事件，具体取决于实现）\n",
    "            #     1: 3,  # 符号对 (A,B) 在单词1中出现3次（或1次事件）\n",
    "            #     2: 2   # 符号对 (A,B) 在单词2中出现2次（或1次事件）\n",
    "            # }\n",
    "            indices[prev_char, char][i] += 1\n",
    "            prev_char = char\n",
    "    return stats, indices\n",
    "\n",
    "\n",
    "def replace_pair(pair, vocab, indices):\n",
    "    \"\"\"\n",
    "    用于将词汇表中所有指定符号对（如 ('A', 'B')）的出现替换为合并后的新符号（如 AB\n",
    "    :param pair: (str,str) 待合并的符号对\n",
    "    :param vocab: List((word,freq))\n",
    "    :param indices: {key:pair,val:{idx,freq}}\n",
    "    :return: 记录所有被修改的单词信息，每个元素为 (j, new_word, word, freq)（j是单词索引，new_word是合并后的新单词，word是原单词，freq是频率）。\n",
    "    \"\"\"\n",
    "    first, second = pair\n",
    "    # 'AB'\n",
    "    pair_str = ''.join(pair)\n",
    "    # 转义反斜杠，避免正则冲突\n",
    "    pair_str = pair_str.replace('\\\\', '\\\\\\\\')\n",
    "    # (?<!...)是正向否定回顾后发断言（Negative Lookbehind Assertion），用于检查当前位置之前的字符是否不满足括号内的模式。\n",
    "    # (?!)是正向否定前瞻断言（Negative Lookahead Assertion），用于检查当前位置之后的字符是否不满足括号内的模式。\n",
    "    # re.escape :用空格连接后的字符串中的所有正则特殊字符转义\n",
    "    pattern = re.compile(r'(?<!\\S)' + re.escape(first + ' ' + second) + r'(?!\\S)')\n",
    "    iterator = indices[pair].items()\n",
    "    changes = []\n",
    "    for j, freq in iterator:\n",
    "        if freq < 1:\n",
    "            continue\n",
    "        word, freq = vocab[j]\n",
    "        # ('A','B') → 'A B'\n",
    "        new_word = ''.join(word)\n",
    "        # 在 string中找到所有匹配 pattern的子串，用 pair_str替换这些子串。\n",
    "        new_word = pattern.sub(pair_str, new_word)\n",
    "        # 转回元组形式（如 'AB' → ('AB',)）\n",
    "        new_word = tuple(new_word.split(' '))\n",
    "\n",
    "        vocab[j] = (new_word, freq)\n",
    "        changes.append((j, new_word, word, freq))\n",
    "    return changes\n",
    "\n",
    "\n",
    "def prune_stats(stats, big_stats, threshold):\n",
    "    \"\"\"\n",
    "    通过删除频率低于阈值的符号对，减小 stats字典的规模，从而提升 max()函数的效率。同时，通过 big_stats保留被剪枝符号对的实际频率，确保统计信息的完整性。\n",
    "    :param stats: 符号对频率统计字典（键：符号对，值：频率)，需要被剪枝优化\n",
    "    :param big_stats: 完整统计字典（键：符号对，值：实际频率），用于保存被剪枝符号对的真实频率。\n",
    "    :param threshold:频率阈值，低于此值的符号对将被剪枝。\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for item, freq in list(stats.items()):\n",
    "        if freq < threshold:\n",
    "            del stats[item]\n",
    "        if freq < 0:\n",
    "            # 处理负频率\n",
    "            big_stats[item] += freq\n",
    "        else:\n",
    "            big_stats[item] = big_stats[item]\n",
    "\n",
    "\n",
    "def learn_bpe(infile_names, outfile_name, num_symbols, min_frequency=2, verbose=False, is_dict=False,\n",
    "              total_symbols=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param infile_names: List[str] 输入文件路径列表，函数从这些文件中读取语料数据以构建词汇表\n",
    "    :param out_files_name: str\t输出文件路径，函数会将学习到的 BPE 子词规则（合并顺序）写入此文件。\n",
    "    :param num_symbols: int 需要学习的子词数量（即最终生成的子词词汇表大小）\n",
    "    :param min_frequency: int 符号对的最小频率阈值。仅当符号对频率 ≥ 此值时，才可能被选为合并对象。\n",
    "    :param verbose: bool 是否启用详细输出模式。若为 True，函数会打印调试信息（如当前处理的符号对、频率等）。\n",
    "    :param is_dict: bool 输入文件是否为“字典格式”。若为 True，文件每行需包含“单词 计数”；否则为普通文本。\n",
    "    :param total_symbols: bool 是否将词内字符和词尾字符视为独立子词。若为 True，会提前扣除这些字符的数量，减少需要学习的子词数量。\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # step 1: 强制标准输入/输出/错误流使用 UTF-8 编码\n",
    "    sys.stderr = codecs.getwriter('UTF-8')(sys.stderr.buffer)\n",
    "    sys.stdout = codecs.getwriter('UTF-8')(sys.stdout.buffer)\n",
    "    sys.stdin = codecs.getreader('UTF-8')(sys.stdin.buffer)\n",
    "\n",
    "    # step 2: 词汇表构建与预处理\n",
    "    vocab = Counter()\n",
    "    for f in infile_names:\n",
    "        sys.stderr.write('Collexting vocab from {}\\n'.format(f))\n",
    "        vocab = update_vocabulary(vocab, f, is_dict)\n",
    "    # (hello,1) -> (('h', 'e', 'l', 'l', 'o', '</w>'),1)\n",
    "    vocab = dict([(tuple(x[:-1]) + (x[-1] + '</w>',), y) for (x, y) in vocab.items()])\n",
    "    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # step 3: 符号对统计和完整信息备份\n",
    "    # stats:{('a','b'):1, ('b','c'):1}\n",
    "    # indices为 {('a','b'):{0:1}, ('b','c'):{1:1}}\n",
    "    stats, indices = get_pair_statistic(sorted_vocab)\n",
    "    big_stats = copy.deepcopy(stats)\n",
    "\n",
    "    # step 4:动态剪枝优化\n",
    "    if total_symbols:\n",
    "        uniq_char_internal = set()\n",
    "        uniq_char_final = set()\n",
    "        for word in vocab:\n",
    "            for char in word[:-1]:\n",
    "                uniq_char_internal.add(char)\n",
    "            uniq_char_final.add(word[-1])\n",
    "        num_symbols -= len(uniq_char_internal) + len(uniq_char_final)\n",
    "\n",
    "    # step 5: 子词合并与规则输出\n",
    "    sys.stderr.write(f'Write vocab file to {outfile_name}')\n",
    "    with codecs.open(outfile_name, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write('#version: 0.2\\n')\n",
    "        # threshold is inspired by Zipfian assumption, but should only affect speed\n",
    "        threshold = max(stats.values()) / 10\n",
    "        for i in range(num_symbols):\n",
    "            most_frequent = max(stats, key=lambda x: (stats[x], x))\n",
    "            # we probably missed the best pair because of pruning; go back to full statistics\n",
    "            if not stats or (i and stats[most_frequent] < threshold):\n",
    "                prune_stats(stats, big_stats, threshold)\n",
    "                stats = copy.deepcopy(big_stats)\n",
    "                most_frequent = max(stats, key=lambda x: (stats[x], x))\n",
    "                # threshold is inspired by Zipfian assumption, but should only affect speed\n",
    "                threshold = stats[most_frequent] * i / (i + 10000.0)\n",
    "                prune_stats(stats, big_stats, threshold)\n",
    "\n",
    "            if stats[most_frequent] < min_frequency:\n",
    "                sys.stderr.write(f'no pair has frequency >= {min_frequency}. Stopping\\n')\n",
    "                break\n",
    "\n",
    "            # 输出模式\n",
    "            if verbose:\n",
    "                sys.stderr.write(f'pair{i}:{most_frequent[0]} {most_frequent[1]}  freq{stats[most_frequent]}\\n')\n",
    "            outfile.write(f'{most_frequent[0]} {most_frequent[1]}\\n')\n",
    "            # 合并替换\n",
    "            changes = replace_pair(most_frequent, sorted_vocab, indices)\n",
    "            # 更新统计信息\n",
    "            update_pair_statistics(most_frequent, changes, stats, indices)\n",
    "            stats[most_frequent] = 0\n",
    "            if not i % 100:\n",
    "                prune_stats(stats, big_stats, threshold)"
   ],
   "id": "5f59d502d3a0aab1",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9.2 Apply BPE\n",
    "Use operations learned with learn_bpe.py to encode a new text."
   ],
   "id": "fd07413a9313f216"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T07:03:09.500731Z",
     "start_time": "2025-09-05T07:03:09.496889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import unicode_literals, division\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "import codecs\n",
    "import io\n",
    "import re\n",
    "import warnings\n",
    "import random"
   ],
   "id": "c1dc5993ea51963",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T07:03:09.576753Z",
     "start_time": "2025-09-05T07:03:09.549891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BPE(object):\n",
    "\n",
    "    def __init__(self, codes, merges=-1, separator='@@', vocab=None, glossaries=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param self:\n",
    "        :param codes:\n",
    "        #version: 0.2  # 版本行（行号1）\n",
    "        a b            # 合并规则1（行号2）\n",
    "        c d            # 合并规则2（行号3）\n",
    "        e f            # 合并规则3（行号4）\n",
    "        g h            # 合并规则4（行号5）\n",
    "        i j            # 合并规则5（行号6）\n",
    "\n",
    "        :param merges:\n",
    "        :param separator:\n",
    "        :param vocab:\n",
    "        :param glossaries:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 将文件指针强制重置回文件的开头位置（位置0）\n",
    "        codes.seek(0)\n",
    "        offset = 1\n",
    "\n",
    "        # check version information\n",
    "        firstline = codes.readline()\n",
    "        if firstline.startswith('#version:'):\n",
    "            # #version: 0.2 -> (0,2)\n",
    "            self.version = tuple([int(x) for x in re.sub(r'(\\.0+)*$', '', firstline.split()[-1]).split(\".\")])\n",
    "            offset += 1\n",
    "        else:\n",
    "            self.version = (0, 1)\n",
    "        # [('a', 'b'), ('c', 'd'), ('e', 'f')]\n",
    "        self.bpe_codes = [tuple(item.strip('\\r\\n').split(' '))\n",
    "                          for (n, item) in enumerate(codes)\n",
    "                          if (n < merges or merges == -1)]\n",
    "        for i, item in enumerate(self.bpe_codes):\n",
    "            if len(item) != 2:\n",
    "                if len(item) != 2:\n",
    "                    sys.stderr.write(f'Error: invalid line {i + offset} in BPE codes file: {\" \".join(item)}\\n')\n",
    "                    sys.stderr.write('The line should exist of exactly two subword units, separated by whitespace\\n')\n",
    "                    sys.exit(1)\n",
    "            codes.seek(0)\n",
    "        # 处理重复合并的：{('e', 'f'): 2,('c', 'd'): 1,('a', 'b'): 0} 《- [(2, ('a','b')), (1, ('c','d')), (0, ('a','b'))]\n",
    "        self.bpe_codes = dict([(code, i) for (i, code) in reversed(list(enumerate(self.bpe_codes)))])\n",
    "        #{'ab':{('a','b')}}\n",
    "        self.bpe_codes_reverse = dict([(pair[0] + pair[1], pair) for pair, i in self.bpe_codes.items()])\n",
    "        # 子词分隔符 un@know->un，known\n",
    "        self.separator = separator\n",
    "        self.vocab = vocab\n",
    "        # 用户指定的术语列表（如[\"COVID-19\", \"NLP\"]），这些术语在分词时需保持完整，不被拆分。\n",
    "        self.glossaries = glossaries if glossaries else []\n",
    "        # ^(COVID-19|NLP)$）\n",
    "        self.glossaries_regex = re.compile('^({})$'.format('|'.join(self.separator))) if glossaries else None\n",
    "        # 缓存已处理的编码结果\n",
    "        self.cache = {}\n",
    "\n",
    "    def process_line(self, line, dropout=0):\n",
    "        \"\"\"\n",
    "         BPE 分词器中处理单行文本的核心方法，其核心目标是在保留原始行前导/尾随空白符的前提下，对中间内容进行 BPE 分词\n",
    "        :param self:\n",
    "        :param line:每一行文本\n",
    "        :param dropout:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        out = \"\"\n",
    "        # 前导空白的总长度\n",
    "        leading_whitespace = len(line) - len(line.lstrip('\\r\\n '))\n",
    "        if leading_whitespace:\n",
    "            # line = \"  hello world\",out=\" \"\n",
    "            out += line[:leading_whitespace]\n",
    "        #  line = \"  hello world\"，则 segment处理 \"hello world\"后可能返回 \"h@@ ello w@@ orld\"\n",
    "        out += self.segment(line, dropout)\n",
    "\n",
    "        trailing_whitespace = len(line) - len(line.rstrip('\\r\\n '))\n",
    "        if trailing_whitespace and trailing_whitespace != len(line):\n",
    "            out += line[-trailing_whitespace:]\n",
    "        return out\n",
    "\n",
    "    def segment(self, sentence, dropout=0):\n",
    "        \"\"\"\n",
    "        对外接口，处理完整的文本行（已按空格分词），保留原始空白符并调用 segment_tokens执行实际分词\n",
    "        :param self:\n",
    "        :param sentence: 已按空格分割的文本行（如 \"hello world\"）\n",
    "        :param dropout:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        segments = self.segment_tokens(sentence.strip('\\r\\n').split(' '), dropout)\n",
    "        return ' '.join(segments)\n",
    "\n",
    "    def segment_tokens(self, tokens, dropout=0):\n",
    "        \"\"\"\n",
    "        对每个 token 执行术语隔离、BPE 合并、子词拼接\n",
    "        :param self:\n",
    "        :param tokens:  [\"hello\", \"world\"]\n",
    "        :param dropout:\n",
    "        :return: 子词列表（如 [\"h@@\", \"ello\", \"w@@\", \"orld\"]\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        for word in tokens:\n",
    "            if not word:\n",
    "                continue\n",
    "            new_word = [out for segment in self._isolate_glossaries(word)\n",
    "                        for out in encode(segment,\n",
    "                                          self.bpe_codes,\n",
    "                                          self.bpe_codes_reverse,\n",
    "                                          self.vocab,\n",
    "                                          self.separator,\n",
    "                                          self.version,\n",
    "                                          self.cache,\n",
    "                                          self.glossaries_regex,\n",
    "                                          dropout\n",
    "                                          )]\n",
    "            for item in new_word[:-1]:\n",
    "                output.append(item + self.separator)\n",
    "            output.append(new_word[-1])\n",
    "            return output\n",
    "\n",
    "    def _isolate_glossary(self, word):\n",
    "        \"\"\"\n",
    "        隔离术语\n",
    "        :param self:\n",
    "        :param word:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        word_segments = [word]\n",
    "        for gloss in self.glossaries:\n",
    "            word_segments = [out_segments for segment in word_segments\n",
    "                             for out_segments in isolate_glossary(segment, gloss)]\n",
    "        return word_segments\n",
    "\n",
    "\n",
    "def encode(orig, bpe_codes, bpe_codes_reverse, vocab, separator, version, cache, glossaries_regex=None, dropout=0):\n",
    "    \"\"\"\n",
    "    输入的原始单词（orig）通过应用 BPE 合并规则，转换为符合词汇表的子词序列\n",
    "    :param orig: 待编码的原始单词（如 \"hello\"）\n",
    "    :param bpe_codes: 正向合并规则字典 {('e', 'f'): 2,('c', 'd'): 1,('a', 'b'): 0}\n",
    "    :param bpe_codes_reverse: 反向合并规则字典（键为合并后的子词，值为对应的合并对，如 {\"ab\": (\"a\", \"b\")}）\n",
    "    :param vocab: 预定义词汇表\n",
    "    :param separator: 子词分隔符（如 @@）\n",
    "    :param version: BPE 版本（控制词尾标记的处理方式，如 (0, 1)或 (0, 2)）\n",
    "    :param cache: 缓存字典（存储已处理单词的编码结果）\n",
    "    :param glossaries_regex: 术语正则表达式\n",
    "    :param dropout: 训练时随机丢弃合并对的概率\n",
    "    :return: 编码后的子词元组（如 (\"h@@\", \"ell\", \"o\")）\n",
    "    \"\"\"\n",
    "    # step 1:缓存与术语处理\n",
    "    if not dropout and orig in cache:\n",
    "        return cache[orig]\n",
    "    if glossaries_regex and glossaries_regex.match(orig):\n",
    "        cache[orig] = (orig,)\n",
    "        return (orig,)\n",
    "\n",
    "    '''\n",
    "    step 2:初始化词尾标记\n",
    "    版本0.1：在词尾添加 </w>（如 \"hello\"→ ['h','e','l','l','o','</w>']）。\n",
    "    版本0.2：在最后一个字符后添加 </w>（如 \"hello\"→ ['h','e','l','l','o</w>']），更一致处理词尾段。\n",
    "    '''\n",
    "    if version == (0, 1):\n",
    "        word = list(orig) + ['</w>']\n",
    "    elif version == (0, 2):\n",
    "        word = list(orig[:-1]) + [orig[-1] + '</w>']\n",
    "\n",
    "    # step 3:迭代合并\n",
    "    while len(word) > 1:\n",
    "        # 生成候选合并对\n",
    "        pairs = [\n",
    "            (bpe_codes[pair], i, pair)  # 生成元组（优先级索引，起始位置，字符对）\n",
    "            for (i, pair) in enumerate(zip(word, word[1:]))  # 遍历所有相邻字符对\n",
    "            if (not dropout or random.random() > dropout)  # 过滤 dropout 丢弃的合并对\n",
    "               and pair in bpe_codes  # 确保合并对存在于 BPE 规则中\n",
    "        ]\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "        # 选择优先级最高的合并对（最小索引）\n",
    "        bigram = min(pairs)[2]\n",
    "\n",
    "        # 确定所有需要合并的位置\n",
    "        positions = [i for (rank, i, pair) in pairs if pair == bigram]\n",
    "\n",
    "        # 执行合并操作\n",
    "        i = 0\n",
    "        new_word = []\n",
    "        bigram_str = ''.join(bigram)\n",
    "        for j in positions:\n",
    "            # 跳过重叠的合并对（如 (x,x,x) 中的第二个 x）\n",
    "            if j < i:\n",
    "                continue\n",
    "            # 添加未合并部分\n",
    "            new_word.extend(word[i:j])\n",
    "            # 添加合并后的子词\n",
    "            new_word.append(bigram_str)\n",
    "            i = j + 2\n",
    "        # 添加剩余未处理部分\n",
    "        new_word.extend(word[i:])\n",
    "        word = new_word\n",
    "\n",
    "        # step 4:词尾处理\n",
    "        if word[-1] == '</w>':\n",
    "            word = word[:-1]\n",
    "        elif word[-1].endswith('</w>'):\n",
    "            #  ['h','e','l','l','o</w>']→ ['h','e','l','l','o']\n",
    "            word[-1] = word[-1][:-4]\n",
    "\n",
    "        # step 5:词汇表检查和拆分\n",
    "        word = tuple(word)\n",
    "        if vocab:\n",
    "            word = check_vocab_and_split(word, bpe_codes_reverse, vocab, separator)\n",
    "\n",
    "        # cache[\"hello\"] = ('he', 'llo')\n",
    "        cache[orig] = word\n",
    "        return word\n",
    "\n",
    "\n",
    "def recursive_split(segment, bpe_codes_reverse, vocab, separator, final=False):\n",
    "    \"\"\"\n",
    "    反向拆分 OOV 子词：对于输入的子词 segment（可能是 OOV），通过反转 BPE 合并规则，逐步拆分为更小的子单元，直到所有子单元可匹配词汇表或无法继续拆分。\n",
    "    :param segment: 待拆分的子词（如 \"hello\"或 \"he@@ llo\"）。\n",
    "    :param bpe_codes_reverse: 反向合并规则字典（键为合并后的子词，值为对应的合并对，如 {\"ab\": (\"a\", \"b\")}）\n",
    "    :param vocab: 预定义词汇表\n",
    "    :param separator: 子词分隔符（如 @@）\n",
    "    :param final: 布尔值，标记是否为词的结尾（影响词尾标记的处理）\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # step 1:查找反向合并树-通过 bpe_codes_reverse反向查找当前 segment的合并来源\n",
    "    try:\n",
    "        if final:\n",
    "            # 词尾模式：查找 \"segment+</w>\" 的合并对（键为合并后的子词）\n",
    "            left, right = bpe_codes_reverse[segment + '</w>']\n",
    "            # 移除词尾标记的后缀（如 \"</w>\" 长度为4）\n",
    "            right = right[:-4]\n",
    "        else:\n",
    "            #非词尾模式\n",
    "            left, right = bpe_codes_reverse[segment]\n",
    "    except KeyError:\n",
    "        # 无合并对可查，无法拆分，返回原segment\n",
    "        yield segment\n",
    "        return\n",
    "\n",
    "        # step 2:处理左子单元\n",
    "    if left + separator in vocab:\n",
    "        yield left\n",
    "    else:\n",
    "        for item in recursive_split(left, bpe_codes_reverse, vocab, separator, final=False):\n",
    "            yield item\n",
    "\n",
    "    # step 3:处理右单元\n",
    "    if (final and right in vocab) or (not final and right + separator in vocab):\n",
    "        yield right\n",
    "    else:\n",
    "        for item in recursive_split(right, bpe_codes_reverse, vocab, separator, final=True):\n",
    "            yield item\n",
    "\n",
    "\n",
    "def check_vocab_and_split(orig, bpe_codes_reverse, vocab, separator):\n",
    "    \"\"\"\n",
    "    BPE 分词器中处理词汇表外子词（OOV, Out-of-Vocabulary）\n",
    "    :param orig: 待处理的子词序列（如 [\"h@@\", \"ell\", \"o\"]）\n",
    "    :param bpe_codes_reverse: 反向合并规则字典（键为合并后的子词，值为对应的合并对，如 {\"ab\": (\"a\", \"b\")}）\n",
    "    :param vocab: 预定义词汇表\n",
    "    :param separator: 子词分隔符（如 @@）\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    # step 1:处理前n-1个子词\n",
    "    for segment in orig[:-1]:\n",
    "        # 检查 \"子词+分隔符\" 是否在词汇表中\n",
    "        if segment + separator in vocab:\n",
    "            out.append(segment)\n",
    "        else:\n",
    "            # 不存在则递归拆分\n",
    "            for item in recursive_split(segment, bpe_codes_reverse, vocab, separator, False):\n",
    "                out.append(item)\n",
    "\n",
    "    # step 2:处理最后一个子词\n",
    "    segment = orig[-1]\n",
    "    if segment in vocab:\n",
    "        out.append(segment)\n",
    "    else:\n",
    "        # 不存在则递归拆分（允许拆分到词尾）\n",
    "        for item in recursive_split(segment, bpe_codes_reverse, vocab, separator, True):\n",
    "            out.append(item)\n",
    "    return out\n",
    "\n",
    "\n",
    "def read_vocabulary(vocab_file, threshold):\n",
    "    \"\"\"\n",
    "    原始词汇表文件中读取单词及其频率，并保留满足频率阈值的单词\n",
    "    :param vocab_file: 词汇表文件对象\n",
    "    :param threshold: 频率阈值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    for line in vocab_file:\n",
    "        word, freq = line.strip('\\r\\n').split(' ')\n",
    "        freq = int(freq)\n",
    "        if threshold == None or freq >= threshold:\n",
    "            vocabulary.add(word)\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "def isolate_glossary(self, word, glossary):\n",
    "    \"\"\"\n",
    "    隔离单词中包含的术语表\n",
    "    :param self: 待处理的原始单词（字符串），可能包含多个术语实例（如 \"1934USABUSA\"）\n",
    "    :param word: ：需要隔离的术语（字符串），如 \"USA\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 判断 word是否完全等同于术语 or 判断 word是否包含术语\n",
    "    if re.match('^' + glossary + '$', word) or not re.search(glossary, word):\n",
    "        return [word]\n",
    "    else:\n",
    "        # re.split(r'(USA)', \"1934USABUSA\")--> ['1934', 'USA', 'B', 'USA']\n",
    "        segments = re.split(r'({})'.format(glossary), word)\n",
    "        segments, ending = segments[:-1], segments[-1]\n",
    "        # 过滤空串\n",
    "        segments = list(filter(None, segments))\n",
    "        # ['1934', 'USA', 'B'] + ['USA'] → ['1934', 'USA', 'B', 'USA']\n",
    "        return segments + [ending.strip('\\r\\n ')] if ending != '' else segments"
   ],
   "id": "ce203db268299355",
   "outputs": [],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

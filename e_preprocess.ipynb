{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 多语言平行语料预处理脚本\n",
    "- 主要功能是下载、清洗、分词（BPE或spacy）、构建词汇表，并将处理后的数据保存为便于模型训练的格式（如pickle)"
   ],
   "id": "555a72aca009236e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T07:58:03.728698Z",
     "start_time": "2025-09-05T07:58:03.721285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import logging\n",
    "import dill as pickle\n",
    "import urllib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import codecs\n",
    "import spacy\n",
    "import torch\n",
    "import tarfile\n",
    "import torchtext.data\n",
    "import torchtext.datasets\n",
    "import argparse\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n"
   ],
   "id": "f874f69c3e3fe6b7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T07:58:03.735387Z",
     "start_time": "2025-09-05T07:58:03.732644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nbimporter\n",
    "# from c_transformer_model import PAD_WORD, UNK_WORD, BOS_WORD, EOS_WORD\n",
    "from d_BPE import learn_bpe, BPE\n",
    "\n",
    "PAD_WORD = '<blank>'  #填充符号\n",
    "UNK_WORD = '<unk>'  #未知词\n",
    "BOS_WORD = '<s>'  #起始符号\n",
    "EOS_WORD = '</s>'  #结束符号"
   ],
   "id": "dd9b4fce6fa345d1",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**机器翻译数据**\n",
    "\n",
    "每个数据源是一个字典，包含三个关键信息：\n",
    "- `url`: 数据文件的下载链接\n",
    "- `trg`: 目标语言(target)文件名\n",
    "- `src`: 源语言(source)文件名"
   ],
   "id": "5d83723b81d8f4d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T07:58:03.786568Z",
     "start_time": "2025-09-05T07:58:03.779771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_TRAIN_DATA_SOURCES = [\n",
    "    {\"url\": \"http://data.statmt.org/wmt17/translation-task/\" \\\n",
    "            \"training-parallel-nc-v12.tgz\",\n",
    "     \"trg\": \"news-commentary-v12.de-en.en\",\n",
    "     \"src\": \"news-commentary-v12.de-en.de\"},\n",
    "    # 注释掉的两个数据源...\n",
    "]\n",
    "\n",
    "_VAL_DATA_SOURCES = [\n",
    "    {\"url\": \"http://data.statmt.org/wmt17/translation-task/dev.tgz\",\n",
    "     \"trg\": \"newstest2013.en\",\n",
    "     \"src\": \"newstest2013.de\"}]\n",
    "\n",
    "_TEST_DATA_SOURCES = [\n",
    "    {\"url\": \"https://storage.googleapis.com/tf-perf-public/\" \\\n",
    "            \"official_transformer/test_data/newstest2014.tgz\",\n",
    "     \"trg\": \"newstest2014.en\",\n",
    "     \"src\": \"newstest2014.de\"}]"
   ],
   "id": "76664e8fcaaef756",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T07:58:03.848133Z",
     "start_time": "2025-09-05T07:58:03.832590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class TqdmUpTo(tqdm):\n",
    "    \"\"\"\n",
    "    进度条\n",
    "    \"\"\"\n",
    "\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        \"\"\"\n",
    "        :param b:已完成块的数量\n",
    "        :param bsize:每个块的大小\n",
    "        :param tsize:任务的总大小\n",
    "        \"\"\"\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        # 目标进度(b * bsize)减去当前已完成进度(self.n)\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def file_exist(dir_name, file_name):\n",
    "    \"\"\"\n",
    "    遍历dir_name,搜索file_name\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(dir_name):\n",
    "        if file_name in files:\n",
    "            return os.path.join(root, file_name)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _download_file(download_dir, url):\n",
    "    \"\"\"\n",
    "    指定 URL 下载文件到本地目录，并支持进度显示和重复下载检查。\n",
    "    \"\"\"\n",
    "    # URL http://data.statmt.org/wmt17/training-parallel-nc-v12.tgz会被分割为 ['http:', '', 'data.statmt.org', 'wmt17', 'training-parallel-nc-v12.tgz']\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if file_exist(download_dir, filename):\n",
    "        sys.stderr.write(f\"Already downloaded:{url} at {filename}\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(f\"Downloading {url} to {filename}\\n\")\n",
    "        # miniters=1：进度条至少每 1 次迭代更新一次,\n",
    "        # desc=filename：进度条前缀显示文件名\n",
    "        with TqdmUpTo(unit=\"B\", unit_scale=True, miniters=1, desc=filename) as t:\n",
    "            # reporthook=t.update_to指定进度回调函数，通过 TqdmUpTo的 update_to方法实时更新进度条。\n",
    "            urllib.request.urlretrieve(url, filename, reporthook=t.update_to)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def download_and_extract(download_dir, url, src_filename, trg_filename):\n",
    "    \"\"\"\n",
    "    从指定 URL 下载压缩语料库，解压后验证源语言（src）和目标语言（trg）文件的存在性\n",
    "    \"\"\"\n",
    "    # step 1: 检查文件是否存在\n",
    "    src_path = file_exist(download_dir, src_filename)\n",
    "    trg_path = file_exist(download_dir, trg_filename)\n",
    "\n",
    "    # step 2:跳过已经存在文件\n",
    "    if src_path and trg_path:\n",
    "        sys.stderr.write(f'Already downloaded and extracted: {src_path} -> {trg_path}\\n')\n",
    "        return src_path, trg_path\n",
    "\n",
    "    # step 3:下载压缩文件\n",
    "    compressed_file = _download_file(download_dir, url)\n",
    "\n",
    "    # step 4: 解压缩文件\n",
    "    sys.stderr.write(f\"Extracting {compressed_file} to {trg_path}\\n\")\n",
    "    # tarfile模块以只读模式打开 gzip 压缩的 tar 文件（r:gz）\n",
    "    with tarfile.open(compressed_file, \"r:gz\") as tar:\n",
    "        tar.extractall(path=download_dir)\n",
    "\n",
    "    # step 5:再次验证解压缩的文件\n",
    "    src_path = file_exist(download_dir, src_filename)\n",
    "    trg_path = file_exist(download_dir, trg_filename)\n",
    "\n",
    "    if src_path and trg_path:\n",
    "        return src_path, trg_path\n",
    "    raise OSError(f\"Download/extraction failed for url {url} to path {download_dir}\")\n",
    "\n",
    "\n",
    "def get_raw_files(raw_dir, sources):\n",
    "    \"\"\"\n",
    "    :param sources:List{url:http://data.statmt.org/wmt17/training-parallel-nc-v12.tgz.\n",
    "                        src:news-commentary-v12.de-en.de,\n",
    "                        trg:news-commentary-v12.de-en.en}\n",
    "    \"\"\"\n",
    "    raw_files = {\"src\": [], \"trg\": []}\n",
    "    for d in sources:\n",
    "        src_file, trg_file = download_and_extract(raw_dir, d[\"url\"], d[\"src\"], d[\"trg\"])\n",
    "        raw_files[\"src\"].append(src_file)\n",
    "        raw_files[\"trg\"].append(trg_file)\n",
    "    return raw_files\n",
    "\n",
    "\n",
    "def mkdir_if_needed(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "\n",
    "def compile_files(\n",
    "        raw_dir: str,\n",
    "        raw_files: dict[str, list[str]],\n",
    "        prefix: str\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    合并多源原始文件为统一的源语言(.src)和目标语言(.trg)文件。\n",
    "\n",
    "    参数:\n",
    "        raw_dir (str): 原始文件根目录（如 `./data/raw`）。\n",
    "        raw_files (dict): 含 \"src\"（源文件路径列表）和 \"trg\"（目标文件路径列表）的字典，两列表长度需一致。\n",
    "        prefix (str): 合并文件前缀（生成如 `raw-train.src` 和 `raw-train.trg`）。\n",
    "\n",
    "    返回:\n",
    "        tuple[str, str]: 合并后的源文件路径和目标文件路径（顺序：(src_fpath, trg_fpath)）。\n",
    "    \"\"\"\n",
    "    #  raw_dir=./data/raw，prefix=train -> ./data/raw/raw-train.src\n",
    "    src_fpath = os.path.join(raw_dir, f\"raw-{prefix}.src\")\n",
    "    trg_fpath = os.path.join(raw_dir, f\"raw-{prefix}.trg\")\n",
    "\n",
    "    if os.path.exists(src_fpath) and os.path.exists(trg_fpath):\n",
    "        sys.stderr.write(f\"Merged files found, skip the merging process.\\n\")\n",
    "        return src_fpath, trg_fpath\n",
    "\n",
    "    # 合并文件\n",
    "    sys.stderr.write(f\"Merge files into two files: {src_fpath} and {trg_fpath}.\\n\")\n",
    "    with open(src_fpath, \"w\") as src_outf, open(trg_fpath, \"w\") as trg_outf:\n",
    "        for src_inf, trg_inf in zip(raw_files[\"src\"], raw_files[\"trg\"]):\n",
    "            sys.stderr.write(f'  Input files: \\n'\n",
    "                             f'    - SRC: {src_inf}, and\\n'\n",
    "                             f'    - TRG: {trg_inf}.\\n'\n",
    "                             )\n",
    "            with open(src_inf, newline='\\n') as src_inf, open(trg_inf, newline='\\n') as trg_inf:\n",
    "                cntr = 0  # 计数器：记录源文件与目标文件的行数差\n",
    "                for i, line in enumerate(src_inf):\n",
    "                    cntr += 1\n",
    "                    # 清理行内空白字符（如 \\r）并添加统一换行符（\\n）\n",
    "                    src_outf.write(line.replace('\\r', ' ').strip() + '\\n')\n",
    "\n",
    "                for j, line in enumerate(trg_inf):\n",
    "                    cntr -= 1\n",
    "                    trg_outf.write(line.replace('\\r', ' ').strip() + '\\n')\n",
    "                    # 验证源文件与目标文件行数是否一致（必须完全匹配)\n",
    "            assert cntr == 0, f'Number of lines in {src_inf} and {trg_inf} are inconsistent.'\n",
    "    return src_fpath, trg_fpath\n",
    "\n",
    "\n",
    "def encode_file(bpe, in_file, out_file):\n",
    "    \"\"\"\n",
    "    原始文本文件（如纯文本语料）通过 BPE 算法转换为分词后的编码文件\n",
    "    :param bpe :BPE编码\n",
    "    :param in_file:原始文本\n",
    "    :param out_file:将编码后的文件写入输出文件\n",
    "    \"\"\"\n",
    "    sys.stderr.write(f\"Read raw content from {in_file} and \\n\" \\\n",
    "                     f\"Write encoded content to {out_file}\\n\")\n",
    "    with codecs.open(in_file, \"r\", encoding=\"utf-8\") as inf:\n",
    "        with codecs.open(out_file, \"w\", encoding=\"utf-8\") as outf:\n",
    "            for line in inf:\n",
    "                outf.write(bpe.process_line(line))\n",
    "\n",
    "\n",
    "def encode_files(bpe, src_in_file, trg_in_file, data_dir, prefix):\n",
    "    \"\"\"\n",
    "    批量处理源语言（src）和目标语言（trg）原始文件，通过 BPE 算法将其转换为模型可处理的结构化编码文件\n",
    "    \"\"\"\n",
    "    src_out_file = os.path.join(data_dir, f\"{prefix}.src\")\n",
    "    trg_out_file = os.path.join(data_dir, f\"{prefix}.trg\")\n",
    "\n",
    "    if os.path.isfile(src_in_file) and os.path.isfile(trg_out_file):\n",
    "        sys.stderr.write(f\"Encode files found, skip the encode process.\\n\")\n",
    "\n",
    "    encode_file(bpe, src_in_file, src_out_file)\n",
    "    encode_file(bpe, trg_in_file, trg_out_file)\n",
    "    return src_out_file, trg_out_file"
   ],
   "id": "38a04a099c3f8590",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T07:58:03.907536Z",
     "start_time": "2025-09-05T07:58:03.893395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-raw_dir\", required=True)\n",
    "    parser.add_argument(\"-data_dir\", required=True)\n",
    "    parser.add_argument('-codes', required=True)\n",
    "    parser.add_argument('-save_data', required=True)\n",
    "    parser.add_argument('-prefix', required=True)\n",
    "    parser.add_argument('-max_len', type=int, default=100)\n",
    "    parser.add_argument('--symbols', '-s', type=int, default=320000, help=\"Vocabulary size\")\n",
    "    # metavar（“元变量名”）用于在命令行帮助信息（-h/--help）中更友好地显示参数的占位符名称。\n",
    "    # metavar会将帮助信息中的占位符从类型名（如 INT）替换为你指定的名称\n",
    "    parser.add_argument(\n",
    "        '--min-frequency', type=int, default=6, metavar='FREQ',\n",
    "        help=\"Stop if no symbol pair has frequency >= FREQ (default: %(default)s))\"\n",
    "    )\n",
    "    # action决定了参数被指定时的行为逻辑:python script.py --dict-input），解析器会为该参数分配一个布尔值 True\n",
    "    parser.add_argument(\n",
    "        '--dict-input', action='store_true',\n",
    "        help=\"If set, input file is interpreted as a dictionary where each line contains a word-count pair\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--separator', type=str, default='@@', metavar='STR',\n",
    "        help=\"Separator between non-final subword units (default: '%(default)s'))\"\n",
    "    )\n",
    "    parser.add_argument('--total-symbols', '-t', action='store_true')\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    # step 1: 数据准备阶段\n",
    "    # 确保原始数据目录（raw_dir）和处理后数据目录（data_dir）存在，避免后续文件操作因目录缺失报错\n",
    "    mkdir_if_needed(opt.raw_dir)\n",
    "    mkdir_if_needed(opt.data_dir)\n",
    "\n",
    "    # 下载并解压原始数据\n",
    "    raw_train = get_raw_files(opt.raw_dir, _TRAIN_DATA_SOURCES)\n",
    "    raw_val = get_raw_files(opt.raw_dir, _VAL_DATA_SOURCES)\n",
    "    raw_test = get_raw_files(opt.raw_dir, _TEST_DATA_SOURCES)\n",
    "\n",
    "    # step 2: 将分散的源文件和目标文件合并为两个大文件（如 train.src和 train.trg）\n",
    "    #prefix='train'，则生成 ./data/raw/raw-train.src（源文件）和 ./data/raw/raw-train.trg\n",
    "    train_src, train_trg = compile_files(opt.raw_dir, raw_train, opt.prefix + '-train')\n",
    "    val_src, val_trg = compile_files(opt.raw_dir, raw_val, opt.prefix + '-val')\n",
    "    test_src, test_trg = compile_files(opt.raw_dir, raw_test, opt.prefix + '-test')\n",
    "\n",
    "    # step 3:BPE编码阶段\n",
    "    # 生成BPE编码文件\n",
    "    opt.codes = os.path.join(opt.data_dir, opt.codes)\n",
    "    if not os.path.isfile(opt.codes):\n",
    "        learn_bpe(raw_train['src'] + raw_train['trg'], opt.codes, opt.symbols, opt.min_frequency, True)\n",
    "\n",
    "    # 初始BPE化分词器\n",
    "    sys.stderr.write(f\"BPE codes prepared.\\n\")\n",
    "    sys.stderr.write(f\"Build up the tokenizer.\\n\")\n",
    "    with codecs.open(opt.codes, encoding=\"utf-8\") as codes:\n",
    "        bpe = BPE(codes, separator=opt.separator)\n",
    "\n",
    "    # 对合并文件进行BPE编码\n",
    "    sys.stderr.write(f\"Encoding ...\\n\")\n",
    "    encode_files(bpe, train_src, train_trg, opt.data_dir, opt.prefix + '-train')\n",
    "    encode_files(bpe, val_src, val_trg, opt.data_dir, opt.prefix + '-val')\n",
    "    encode_files(bpe, test_src, test_trg, opt.data_dir, opt.prefix + '-test')\n",
    "\n",
    "    # step 4: 数据集构建与数据保存\n",
    "    field = torchtext.data.Field(\n",
    "        tokenize=str.split,  # 初始分词\n",
    "        lower=True,\n",
    "        pad_token=PAD_WORD,  # 填充标记\n",
    "        init_token=BOS_WORD,  # 句首标记\n",
    "        eos_token=EOS_WORD  #句尾标记\n",
    "    )\n",
    "\n",
    "    # 构建翻译数据集\n",
    "    fields = (field, field)\n",
    "    MAX_LEN = opt.max_len\n",
    "\n",
    "    def filter_examples_with_length(x):\n",
    "        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "\n",
    "    train = Multi30k(\n",
    "        fields=fields,\n",
    "        path=os.path.join(opt.data_dir, opt.prefix + '-train'),\n",
    "        exts=('.src', '.trg'),\n",
    "        filter_func=filter_examples_with_length,\n",
    "    )\n",
    "\n",
    "    # 生成词汇表（基于BPE编码后的数据）\n",
    "    from itertools import chain\n",
    "    # chain：将多个可迭代对象（如 train.src和 train.trg）连接成一个连续的迭代器。此处用于合并源语言和目标语言的语料，生成统一的语料集合\n",
    "    # build_vocab方法：torchtext.data.Field类的方法，用于基于语料统计词频并生成词汇表\n",
    "    field.build_vocab(chain(train.src, train.trg), min_freq=2)\n",
    "\n",
    "    #  field： 保存数据字段的处理规则（如分词、填充标记）\n",
    "    data = {'settings': opt, 'vocab': field, }\n",
    "    opt.save_data = os.path.join(opt.data_dir, opt.save_data)\n",
    "\n",
    "    print('[Info] Dumping the processed data to pickle file', opt.save_data)\n",
    "    pickle.dump(data, open(opt.save_data, 'wb'))"
   ],
   "id": "5cf41a32ddaca07c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T08:47:20.261266Z",
     "start_time": "2025-09-05T08:47:20.247601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_wo_bpe(opt: argparse.Namespace = None):\n",
    "    \"\"\"\n",
    "    支持命令行与传参两种调用方式\n",
    "    示例:\n",
    "        python preprocess.py -lang_src de -lang_trg en -save_data multi30k_de_en.pkl -share_vocab\n",
    "    \"\"\"\n",
    "    if opt is None:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('-lang_src', required=True,\n",
    "                            choices=['de', 'el', 'en', 'es', 'fr', 'it', 'lt', 'nb', 'nl', 'pt'])\n",
    "        parser.add_argument('-lang_trg', required=True,\n",
    "                            choices=['de', 'el', 'en', 'es', 'fr', 'it', 'lt', 'nb', 'nl', 'pt'])\n",
    "        parser.add_argument('-save_data', required=True)\n",
    "        parser.add_argument('-max_len', type=int, default=100)\n",
    "        parser.add_argument('-min_word_count', type=int, default=3)\n",
    "        parser.add_argument('-keep_case', action='store_true')\n",
    "        parser.add_argument('-share_vocab', action='store_true')\n",
    "        opt = parser.parse_args()\n",
    "    else:\n",
    "        required_args = ['lang_src', 'lang_trg', 'save_data']\n",
    "        for arg in required_args:\n",
    "            if not hasattr(opt, arg):\n",
    "                raise ValueError(f\"Missing required argument: {arg}\")\n",
    "\n",
    "    # 检查自定义数据是否启用\n",
    "    assert not any([opt.data_src, opt.data_trg]), 'Custom data input is not supported now.'\n",
    "\n",
    "    # 加载语言模型\n",
    "    lang_to_spacy_model = {\n",
    "        'de': 'de_core_news_sm',  # 德语模型\n",
    "        'en': 'en_core_web_sm',  # 英语模型\n",
    "    }\n",
    "    src_lang_model = spacy.load(lang_to_spacy_model[opt.lang_src])\n",
    "    trg_lang_model = spacy.load(lang_to_spacy_model[opt.lang_trg])\n",
    "\n",
    "    # 分词函数\n",
    "    def tokenize_src(text):\n",
    "        return [tok.text for tok in src_lang_model.tokenizer(text)]\n",
    "\n",
    "    def tokenize_trg(text):\n",
    "        return [tok.text for tok in trg_lang_model.tokenizer(text)]\n",
    "\n",
    "    # 加载 Multi30k 数据集\n",
    "    train_data = Multi30k(split='train', language_pair=(opt.lang_src, opt.lang_trg))\n",
    "    val_data = Multi30k(split='valid', language_pair=(opt.lang_src, opt.lang_trg))\n",
    "    test_data = Multi30k(split=\"test\", language_pair=(opt.lang_src, opt.lang_trg))\n",
    "\n",
    "    # 过滤过长句子\n",
    "    # MAX_LEN = opt.max_len\n",
    "    # train_data = [ex for ex in train_data if len(ex[0]) <= MAX_LEN and len(ex[1]) <= MAX_LEN]\n",
    "    # val_data = [ex for ex in val_data if len(ex[0]) <= MAX_LEN and len(ex[1]) <= MAX_LEN]\n",
    "    # test_data = [ex for ex in test_data if len(ex[0]) <= MAX_LEN and len(ex[1]) <= MAX_LEN]\n",
    "\n",
    "    # 构建词汇表\n",
    "    MIN_FREQ = opt.min_word_count\n",
    "\n",
    "    def yield_tokens(data_iter, tokenizer):\n",
    "        for src, trg in data_iter:\n",
    "            yield tokenizer(src)\n",
    "            yield tokenizer(trg)\n",
    "\n",
    "    # 构建源语言词汇表\n",
    "    src_tokenizer = get_tokenizer(tokenize_src)\n",
    "    src_vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(train_data, src_tokenizer),\n",
    "        min_freq=MIN_FREQ,\n",
    "        specials=[PAD_WORD, BOS_WORD, EOS_WORD],\n",
    "        special_first=True\n",
    "    )\n",
    "    # 当遇到未知单词时返回该索引\n",
    "    src_vocab.set_default_index(src_vocab[PAD_WORD])\n",
    "\n",
    "    # 构建目标语言词汇表\n",
    "    trg_tokenizer = get_tokenizer(tokenize_trg)\n",
    "    trg_vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(train_data, trg_tokenizer),\n",
    "        min_freq=MIN_FREQ,\n",
    "        specials=[PAD_WORD, BOS_WORD, EOS_WORD],\n",
    "        special_first=True\n",
    "    )\n",
    "    trg_vocab.set_default_index(trg_vocab[PAD_WORD])\n",
    "\n",
    "    # 共享词汇表\n",
    "    def yield_tokens(data_iter, tokenizer):\n",
    "        for src, trg in data_iter:\n",
    "            yield tokenizer(src)\n",
    "            yield tokenizer(trg)\n",
    "\n",
    "    # 构建共享词汇表（如果启用）\n",
    "    if opt.share_vocab:\n",
    "        print(\"[Info] Merging source and target vocabularies...\")\n",
    "        shared_vocab = build_vocab_from_iterator(\n",
    "            yield_tokens(train_data, tokenize_src),\n",
    "            min_freq=MIN_FREQ,\n",
    "            specials=[PAD_WORD, BOS_WORD, EOS_WORD],\n",
    "            special_first=True\n",
    "        )\n",
    "        shared_vocab.set_default_index(shared_vocab[PAD_WORD])\n",
    "        src_vocab = shared_vocab\n",
    "        trg_vocab = shared_vocab\n",
    "        print(f\"[Info] Merged vocabulary size: {len(shared_vocab)}\")\n",
    "\n",
    "    # 保存数据\n",
    "    data = {\n",
    "        'settings': opt,\n",
    "        'vocab': {'src': src_vocab, 'trg': trg_vocab},\n",
    "        'train': train_data,\n",
    "        'valid': val_data,\n",
    "        'test': test_data\n",
    "    }\n",
    "\n",
    "    print(f\"[Info] Saving processed data to {opt.save_data}\")\n",
    "    pickle.dump(data, open(opt.save_data, 'wb'))"
   ],
   "id": "5a75493dc7c11987",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.2 Download the spacy language model.\n",
    "- python -m spacy download en\n",
    "- python -m spacy download de"
   ],
   "id": "cd03e77e6bf5e615"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.3 Preprocess the data with torchtext and spacy.\n",
    "- python preprocess.py -lang_src de -lang_trg en -share_vocab -save_data m30k_deen_shr.pkl"
   ],
   "id": "502025af196a47e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T08:47:32.823805Z",
     "start_time": "2025-09-05T08:47:23.490810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "\n",
    "opt = argparse.Namespace(\n",
    "    lang_src='de',  # 源语言（德语）\n",
    "    lang_trg='en',  # 目标语言（英语）\n",
    "    save_data='multi30k_de_en.pkl',  # 保存路径\n",
    "    data_src=None,\n",
    "    data_trg=None,\n",
    "    max_len=100,\n",
    "    min_word_count=3,\n",
    "    keep_case=True,\n",
    "    share_vocab=True,\n",
    ")\n",
    "\n",
    "main_wo_bpe(opt)"
   ],
   "id": "3689303b6069c260",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zengmj/.conda/envs/transfomer/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Merging source and target vocabularies...\n",
      "[Info] Merged vocabulary size: 10264\n",
      "[Info] Saving processed data to multi30k_de_en.pkl\n"
     ]
    }
   ],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
